{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import recognitionTypes.hardCodedRecognition as hard_coded\n",
    "from recognitionTypes.classificatorsRecognition import ClassificatorRecognition as classificator\n",
    "from recognitionTypes.neuralNetworkRecognition import NeuralRecognition as neuralRecognition\n",
    "import cv2 as cv2\n",
    "import mediapipe as mp\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/11\n",
      " 96/136 [====================>.........] - ETA: 0s - loss: 2.1168 - accuracy: 0.2563\n",
      "Epoch 1: val_loss improved from inf to 1.51659, saving model to keypoint_classifier\\keypoint_classifier.hdf5\n",
      "136/136 [==============================] - 1s 2ms/step - loss: 1.9894 - accuracy: 0.3124 - val_loss: 1.5166 - val_accuracy: 0.5268\n",
      "Epoch 2/11\n",
      " 94/136 [===================>..........] - ETA: 0s - loss: 1.1880 - accuracy: 0.5592\n",
      "Epoch 2: val_loss improved from 1.51659 to 0.72942, saving model to keypoint_classifier\\keypoint_classifier.hdf5\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 1.0698 - accuracy: 0.6035 - val_loss: 0.7294 - val_accuracy: 0.7502\n",
      "Epoch 3/11\n",
      " 94/136 [===================>..........] - ETA: 0s - loss: 0.6158 - accuracy: 0.7872\n",
      "Epoch 3: val_loss improved from 0.72942 to 0.48192, saving model to keypoint_classifier\\keypoint_classifier.hdf5\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.5843 - accuracy: 0.8025 - val_loss: 0.4819 - val_accuracy: 0.8464\n",
      "Epoch 4/11\n",
      " 97/136 [====================>.........] - ETA: 0s - loss: 0.3999 - accuracy: 0.8732\n",
      "Epoch 4: val_loss improved from 0.48192 to 0.30485, saving model to keypoint_classifier\\keypoint_classifier.hdf5\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.3774 - accuracy: 0.8820 - val_loss: 0.3048 - val_accuracy: 0.9103\n",
      "Epoch 5/11\n",
      " 95/136 [===================>..........] - ETA: 0s - loss: 0.2762 - accuracy: 0.9115\n",
      "Epoch 5: val_loss improved from 0.30485 to 0.23012, saving model to keypoint_classifier\\keypoint_classifier.hdf5\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.2614 - accuracy: 0.9166 - val_loss: 0.2301 - val_accuracy: 0.9246\n",
      "Epoch 6/11\n",
      " 93/136 [===================>..........] - ETA: 0s - loss: 0.2057 - accuracy: 0.9276\n",
      "Epoch 6: val_loss improved from 0.23012 to 0.18440, saving model to keypoint_classifier\\keypoint_classifier.hdf5\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1973 - accuracy: 0.9313 - val_loss: 0.1844 - val_accuracy: 0.9339\n",
      "Epoch 7/11\n",
      " 93/136 [===================>..........] - ETA: 0s - loss: 0.1687 - accuracy: 0.9350\n",
      "Epoch 7: val_loss improved from 0.18440 to 0.15545, saving model to keypoint_classifier\\keypoint_classifier.hdf5\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.1600 - accuracy: 0.9412 - val_loss: 0.1555 - val_accuracy: 0.9426\n",
      "Epoch 8/11\n",
      " 91/136 [===================>..........] - ETA: 0s - loss: 0.1353 - accuracy: 0.9505\n",
      "Epoch 8: val_loss improved from 0.15545 to 0.13017, saving model to keypoint_classifier\\keypoint_classifier.hdf5\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1350 - accuracy: 0.9498 - val_loss: 0.1302 - val_accuracy: 0.9500\n",
      "Epoch 9/11\n",
      " 95/136 [===================>..........] - ETA: 0s - loss: 0.1171 - accuracy: 0.9572\n",
      "Epoch 9: val_loss improved from 0.13017 to 0.10917, saving model to keypoint_classifier\\keypoint_classifier.hdf5\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.1138 - accuracy: 0.9589 - val_loss: 0.1092 - val_accuracy: 0.9579\n",
      "Epoch 10/11\n",
      " 92/136 [===================>..........] - ETA: 0s - loss: 0.0965 - accuracy: 0.9654\n",
      "Epoch 10: val_loss improved from 0.10917 to 0.09060, saving model to keypoint_classifier\\keypoint_classifier.hdf5\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 0.0961 - accuracy: 0.9653 - val_loss: 0.0906 - val_accuracy: 0.9713\n",
      "Epoch 11/11\n",
      " 94/136 [===================>..........] - ETA: 0s - loss: 0.0793 - accuracy: 0.9776\n",
      "Epoch 11: val_loss improved from 0.09060 to 0.07788, saving model to keypoint_classifier\\keypoint_classifier.hdf5\n",
      "136/136 [==============================] - 0s 1ms/step - loss: 0.0751 - accuracy: 0.9798 - val_loss: 0.0779 - val_accuracy: 0.9750\n"
     ]
    }
   ],
   "source": [
    "neural = neuralRecognition()\n",
    "neural.trainNetwork(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mainprogram(recognitionType):\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    mpHands = mp.solutions.hands\n",
    "    hands = mpHands.Hands( max_num_hands=1, min_detection_confidence=0.9, static_image_mode=False)\n",
    "    mpDraw = mp.solutions.drawing_utils\n",
    "\n",
    "    while True:\n",
    "        success, img = cap.read()\n",
    "        imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.flip(img,1)\n",
    "        results = hands.process(img)\n",
    "        h, w, c = img.shape\n",
    "        cv2.rectangle(img, (20,20), (300,400), (255,0,0), 2)\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            for handLms in results.multi_hand_landmarks:\n",
    "                for id, lm in enumerate(handLms.landmark):\n",
    "                    cx, cy = int(lm.x * w), int(lm.y * h)\n",
    "\n",
    "                landMarkVect = handLms.landmark\n",
    "                hard_coded.checkPointsRange(landMarkVect, w, h)\n",
    "\n",
    "                gestureName = recognitionType.getGestureName(landMarkVect, w, h)\n",
    "\n",
    "                mpDraw.draw_landmarks(img, handLms, mpHands.HAND_CONNECTIONS)\n",
    "                cv2.putText(img, gestureName, (10, 70), cv2.FONT_ITALIC, 1, (0, 0, 255), 1)\n",
    "\n",
    "        cv2.imshow(\"Image\", img)\n",
    "        k = cv2.waitKey(1) & 0xFF\n",
    "        if k == 27:\n",
    "            cv2.destroyAllWindows()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mainprogram(neural)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "96d6fdddf3a320460093ab46ca048a27a0b73998106774fcafa1a2fc661e57f5"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
