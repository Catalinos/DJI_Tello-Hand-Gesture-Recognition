{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import recognitionTypes.hardCodedRecognition as hard_coded\n",
    "from recognitionTypes.classificatorsRecognition import ClassificatorRecognition as classificator\n",
    "from recognitionTypes.neuralNetworkRecognition import NeuralRecognition as neuralRecognition\n",
    "import cv2 as cv2\n",
    "import mediapipe as mp\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural = neuralRecognition()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural.load_saved_model(\"keypoint_classifier/keypoint_classifier.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NeuralRecognition' object has no attribute 'history'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\Git\\HandGestNotebook\\Untitled-1.ipynb Cell 4'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Git/HandGestNotebook/Untitled-1.ipynb#ch0000005?line=0'>1</a>\u001b[0m neural\u001b[39m.\u001b[39;49mplot_model_acuracy()\n",
      "File \u001b[1;32md:\\Git\\HandGestNotebook\\recognitionTypes\\neuralNetworkRecognition.py:82\u001b[0m, in \u001b[0;36mNeuralRecognition.plot_model_acuracy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     <a href='file:///d%3A/Git/HandGestNotebook/recognitionTypes/neuralNetworkRecognition.py?line=80'>81</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot_model_acuracy\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m---> <a href='file:///d%3A/Git/HandGestNotebook/recognitionTypes/neuralNetworkRecognition.py?line=81'>82</a>\u001b[0m     history \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhistory\n\u001b[0;32m     <a href='file:///d%3A/Git/HandGestNotebook/recognitionTypes/neuralNetworkRecognition.py?line=82'>83</a>\u001b[0m     plt\u001b[39m.\u001b[39mplot(history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     <a href='file:///d%3A/Git/HandGestNotebook/recognitionTypes/neuralNetworkRecognition.py?line=83'>84</a>\u001b[0m     plt\u001b[39m.\u001b[39mplot(history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mval_accuracy\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NeuralRecognition' object has no attribute 'history'"
     ]
    }
   ],
   "source": [
    "neural.plot_model_acuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mainprogram(recognitionType):\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    mpHands = mp.solutions.hands\n",
    "    hands = mpHands.Hands( max_num_hands=1, min_detection_confidence=0.9, static_image_mode=False)\n",
    "    mpDraw = mp.solutions.drawing_utils\n",
    "\n",
    "    while True:\n",
    "        success, img = cap.read()\n",
    "        imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.flip(img,1)\n",
    "        results = hands.process(img)\n",
    "        h, w, c = img.shape\n",
    "        cv2.rectangle(img, (20,20), (300,400), (255,0,0), 2)\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            for handLms in results.multi_hand_landmarks:\n",
    "                for id, lm in enumerate(handLms.landmark):\n",
    "                    cx, cy = int(lm.x * w), int(lm.y * h)\n",
    "\n",
    "                landMarkVect = handLms.landmark\n",
    "                hard_coded.checkPointsRange(landMarkVect, w, h)\n",
    "\n",
    "                gestureName = recognitionType.getGestureName(landMarkVect, w, h)\n",
    "\n",
    "                mpDraw.draw_landmarks(img, handLms, mpHands.HAND_CONNECTIONS)\n",
    "                cv2.putText(img, gestureName, (10, 70), cv2.FONT_ITALIC, 1, (0, 0, 255), 1)\n",
    "\n",
    "        cv2.imshow(\"Image\", img)\n",
    "        k = cv2.waitKey(1) & 0xFF\n",
    "        if k == 27:\n",
    "            cv2.destroyAllWindows()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainprogram(neural)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "96d6fdddf3a320460093ab46ca048a27a0b73998106774fcafa1a2fc661e57f5"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
